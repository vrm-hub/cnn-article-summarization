{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"validation.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8417c823d1c6e263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_df['article_tokens'] = train_df['article'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "train_df['highlights_tokens'] = train_df['highlights'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=150))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ccbf6b852909ab6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_df['article_tokens'] = val_df['article'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "val_df['highlights_tokens'] = val_df['highlights'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=150))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29722e4c4b5f7f8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df['article_tokens'] = test_df['article'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "test_df['highlights_tokens'] = test_df['highlights'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=150))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c404691a3e8c8a1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, article_tokens, highlights_tokens):\n",
    "        self.article_tokens = article_tokens\n",
    "        self.highlights_tokens = highlights_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.article_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.article_tokens[idx], 'labels': self.highlights_tokens[idx]}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f3b0f23584c67d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset(train_df['article_tokens'].tolist(), train_df['highlights_tokens'].tolist())\n",
    "val_dataset = SummarizationDataset(val_df['article_tokens'].tolist(), val_df['highlights_tokens'].tolist())\n",
    "test_dataset = SummarizationDataset(test_df['article_tokens'].tolist(), test_df['highlights_tokens'].tolist())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c9e566b0f267e75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=150)  \n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414319d908371aad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87f5748755bfde88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(3):  \n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            val_losses.append(outputs.loss.item())\n",
    "\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f\"Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "model.save_pretrained('./')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d07f7f1c9c9c1ee9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    article_text = row['article']\n",
    "    reference_summary = row['highlights']\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(**inputs)\n",
    "        predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    references.append(reference_summary)\n",
    "    predictions.append(predicted_summary)\n",
    "\n",
    "rouge_scores = scorer.score(references, predictions)\n",
    "meteor_scores = [meteor_score([ref], pred) for ref, pred in zip(references, predictions)]\n",
    "\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "print(\"METEOR Scores:\", meteor_scores)\n",
    "\n",
    "plt.bar(['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'METEOR'], [rouge_scores['rouge1'][2], rouge_scores['rouge2'][2], rouge_scores['rougeL'][2], sum(meteor_scores) / len(meteor_scores)])\n",
    "plt.ylabel('Score')\n",
    "plt.title('Evaluation Metrics')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "874db7ba85d6f228"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
