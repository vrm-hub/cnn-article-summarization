{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/acehunter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 0 - 638ba1352bdf405a8f5bd681d7fe5c928686afff.story - time taken: 1.1711809635162354\n",
      "Processed file 1 - f9f9601180ab3278165d936821e8f145659997f3.story - time taken: 1.0770392417907715\n",
      "Processed file 2 - 80ec0efb252ec4470aee44482d1e196111b5780b.story - time taken: 0.6344387531280518\n",
      "Processed file 3 - 8435150be66ea9792999dfc233cc690f9c2fe2d0.story - time taken: 0.5648210048675537\n",
      "Processed file 4 - 1444cf4d1832507a29a98529c2cd1a41f0154b52.story - time taken: 1.8994011878967285\n",
      "Processed file 5 - c302cdf2cafb85f08e0eb964f23736820b66a3ac.story - time taken: 0.8048110008239746\n",
      "Processed file 6 - 95878b495e8b94f85e04e49ed84b2761d675f765.story - time taken: 1.1487658023834229\n",
      "Processed file 7 - 4b8a784508a50f1fb8bcf799004af94cafe875b5.story - time taken: 0.644155740737915\n",
      "Processed file 8 - 88a754dc1a3f0cbce82bf7b8ec32f6bd62ac6d39.story - time taken: 1.4344141483306885\n",
      "Processed file 9 - d53cc805221115ab83e196f5b463f6586ea2cefe.story - time taken: 2.042681932449341\n",
      "Processed file 10 - ae165e4fc6076a5b1758a7e88e8ffcfea2c80893.story - time taken: 0.9538578987121582\n",
      "Processed file 11 - 399d95fbaa7715199ad9bf25d0c004f705501941.story - time taken: 0.37090301513671875\n",
      "Processed file 12 - b2441a42e474f99491c835a9e03f2a246ab5dd6d.story - time taken: 0.9986720085144043\n",
      "Processed file 13 - 089a94de6b81069cd555b5a7ec215d3e8887bb34.story - time taken: 0.36332106590270996\n",
      "Processed file 14 - f1a18354d77a924ee24ef2b17cc43f0e40a0e24f.story - time taken: 0.8393280506134033\n",
      "Processed file 15 - 113e784f86fb0bc571e7b220c180b143eb7b2a11.story - time taken: 0.3916018009185791\n",
      "Processed file 16 - c398f7844f6876a87fb27dde8568d19c41e2d42e.story - time taken: 2.266611099243164\n",
      "Processed file 17 - a8abcbc18240456234aac9c0a893bfd4652a6052.story - time taken: 0.861191987991333\n",
      "Processed file 18 - b831f8b72293612c8e586f590a59a2374af89ae5.story - time taken: 0.5279319286346436\n",
      "Processed file 19 - 5c9e2f75ce1ee26797b39967aa7b1cc02f0664bf.story - time taken: 2.909578800201416\n",
      "Processed file 20 - 5c546d8d0e6ac20efe2ba4ee18ba452d6825b72a.story - time taken: 1.5528059005737305\n",
      "Processed file 21 - b0ff7f74cfcd5b8db155054ec37165aad0c6986d.story - time taken: 0.6086351871490479\n",
      "Processed file 22 - 0693931923c256663ef5a667fe0f2c1e8df0be05.story - time taken: 0.7173550128936768\n",
      "Processed file 23 - 32a01b4c2f6f61eed9483e50e9959664b5d73904.story - time taken: 2.3976099491119385\n",
      "Processed file 24 - 7ae87e7f8f9cfb8942e0cce9f776731da65985fb.story - time taken: 1.1458508968353271\n",
      "Processed file 25 - 3c00ed335258b714f86f4e5de07890feac213e88.story - time taken: 1.4756050109863281\n",
      "Processed file 26 - b5352d0a72d5e7ec81cf57c75dbf20493c26e305.story - time taken: 1.3635900020599365\n",
      "Processed file 27 - d576a0556fa682ffb00d87e8f421a00f48a04b6d.story - time taken: 1.0310688018798828\n",
      "Processed file 28 - 6151cc2f48b13a1a7c487874ab4f3f77a30b7443.story - time taken: 1.0543231964111328\n",
      "Processed file 29 - 7050bd8773aad092d6b9ffc21f8d27c48d4441d2.story - time taken: 0.5715508460998535\n",
      "Processed file 30 - db2692b605c0cb390e802a93db4e3d592c6d5032.story - time taken: 1.1354038715362549\n",
      "Processed file 31 - 1f36db55e8199bceb836d3f0af792f9da3139b8d.story - time taken: 1.2890222072601318\n",
      "Processed file 32 - e23fdedfa84dae156eec83e5d6bb923770bc2eea.story - time taken: 0.795522928237915\n",
      "Processed file 33 - 47b691352c8d6d7b51284dd00315e2a33798fc3d.story - time taken: 0.9619359970092773\n",
      "Processed file 34 - 2812c3e0daeb89d479e74026e68978004b92bba6.story - time taken: 0.9956531524658203\n",
      "Processed file 35 - fb6fb0ab37b59438b7a1bfb5b31ea3c57c7e7a74.story - time taken: 0.6263000965118408\n",
      "Processed file 36 - 8202de0d1bdbba8803a9747913dada7489655cfd.story - time taken: 0.5974960327148438\n",
      "Processed file 37 - cd8fb20c0ada3ef7ff224e8f0602ef83d36830ac.story - time taken: 1.3338072299957275\n",
      "Processed file 38 - 669421630efaddb2b43c3c95ade4baa1cd5ddb6d.story - time taken: 1.3986608982086182\n",
      "Processed file 39 - e827e7f4d7047df04692f85478fea58b9ece1c42.story - time taken: 2.438318967819214\n",
      "Processed file 40 - 552e3dd70ee5a273865a94ab4a4c8089215635a3.story - time taken: 0.38389015197753906\n",
      "Processed file 41 - 6967fc649222601ee53639e06ed40a5f71d17ba5.story - time taken: 0.7142078876495361\n",
      "Processed file 42 - d2e8dad10a691e5fab9e1c301a54de735dd4c315.story - time taken: 1.763200044631958\n",
      "Processed file 43 - 2060cfbbd009f222cd17cda22796f914611a2c5c.story - time taken: 0.5641562938690186\n",
      "Processed file 44 - 6afe7b5c03972be1d5c47b32d5bc513660573b2a.story - time taken: 0.846282958984375\n",
      "Processed file 45 - 2c117855ca4d1ca6a0244279f48e78b715fde93e.story - time taken: 0.3621189594268799\n",
      "Processed file 46 - a0ee987d3de53da5a5a4b7978c0c0570d1408a8a.story - time taken: 1.0288197994232178\n",
      "Processed file 47 - 2acfb9a655af56399b46944e5d4f4ee79e2115e8.story - time taken: 0.3089480400085449\n",
      "Processed file 48 - 363439496e6bd068f580bbd703f9a079544688d5.story - time taken: 0.37375688552856445\n",
      "Processed file 49 - 597488c7dcfed781edaf5371fd39926efc1081e5.story - time taken: 0.9228153228759766\n",
      "Processed file 50 - c1ce4db5b281b9f9e863ee519c52908b08160cef.story - time taken: 1.0699546337127686\n",
      "Processed file 51 - e96e0821c8519db61d8ac2813feed4af6c964a95.story - time taken: 0.32178401947021484\n",
      "Processed file 52 - ef0054d50d7921ec1641d72c04963973de8c44cf.story - time taken: 0.9079430103302002\n",
      "Processed file 53 - ad2e90052e15364c93011386322f6a5007314348.story - time taken: 0.6092820167541504\n",
      "Processed file 54 - 0e2ce5de6d7737cf1f55513d04ef69e6e8cbfa62.story - time taken: 0.6496813297271729\n",
      "Processed file 55 - e1b4835e5c87bb9f49f1796e5df27027d33b3ca2.story - time taken: 0.36497998237609863\n",
      "Processed file 56 - c0c3a18f12eceb6e5237902de8eb73a377ee480b.story - time taken: 0.6542608737945557\n",
      "Processed file 57 - c39746007c6dd5df699209ff0fc4fd18be1d3bca.story - time taken: 0.6992011070251465\n",
      "Processed file 58 - 4a0caecbc2bdd104e56828bc68a8244f1b3743f6.story - time taken: 0.7486870288848877\n",
      "Processed file 59 - d62a9075795614e1844585abfcc5fbf08eaefb19.story - time taken: 1.1031520366668701\n",
      "Processed file 60 - 561c64b7ee305ab0b9e2cad22f712c0df73e6f29.story - time taken: 0.38498997688293457\n",
      "Processed file 61 - f080abbbdb72f3ec55803a5141bc85ecf573c62d.story - time taken: 1.0425992012023926\n",
      "Processed file 62 - 9902d8b54cb8a7072fc01d01a0e728a613ca0a96.story - time taken: 1.3895738124847412\n",
      "Processed file 63 - cb5d76fca8be36980ddf6047a7a7460519be7a64.story - time taken: 0.6553421020507812\n",
      "Processed file 64 - 1ec999fb120bba00706f6ab6a469ab40a0b9e282.story - time taken: 0.7241129875183105\n",
      "Processed file 65 - ac8a72660d0da726ef73d86ff384cb02f32149e9.story - time taken: 0.6469731330871582\n",
      "Processed file 66 - 47588b2c1a4142c5d01d405b58ef63fd2a013ad7.story - time taken: 1.240027904510498\n",
      "Processed file 67 - ba15c606372a6863e96c0d43b7dd26b9cfda5737.story - time taken: 0.7242581844329834\n",
      "Processed file 68 - 0d7f5a1f6ada1f9fa8ac702e69152efd09b1fa0f.story - time taken: 0.4569070339202881\n",
      "Processed file 69 - d3e355f0ad0c4daf0745f1e1fd13f1423dc95633.story - time taken: 0.17066407203674316\n",
      "Processed file 70 - 7bf968e6e382f9bb6819f834c8773d8aabbd739d.story - time taken: 0.2442009449005127\n",
      "Processed file 71 - d212e4970dad4ce70621488ffc25ee1270715f47.story - time taken: 2.142390727996826\n",
      "Processed file 72 - 5a39a29f30811fcf3e26c67119284b7a206e1fb8.story - time taken: 1.0177958011627197\n",
      "Processed file 73 - 68a072d7d52f983804912220dcfa0fa4e762523c.story - time taken: 0.9772567749023438\n",
      "Processed file 74 - cfddd8298285ee3d3a72590a54fd70379460fc91.story - time taken: 0.4960591793060303\n",
      "Processed file 75 - f0b337644a4ea7d97c1c9c071f83a12af300e1d0.story - time taken: 1.2295448780059814\n",
      "Processed file 76 - dd8b106a3a2b16c718083ce8ff78f7e6e958b497.story - time taken: 1.8177599906921387\n",
      "Processed file 77 - 0b5efe6892457a87f15de5521efae6a5e1328ea3.story - time taken: 2.594212055206299\n",
      "Processed file 78 - 11a58792bb05cd081f991edce6a710ffcc85d98b.story - time taken: 0.8630321025848389\n",
      "Processed file 79 - 0c550db9045dd09d080a98b4addae224c8d24bca.story - time taken: 0.6870260238647461\n",
      "Processed file 80 - 55518e92a5c10e79854049ca454b13525b12a447.story - time taken: 1.0656499862670898\n",
      "Processed file 81 - 1ea4c875fd81b997acf9ba7344fde4710431e2e2.story - time taken: 0.634998083114624\n",
      "Processed file 82 - 6504546a715f7dfa7a66a5d4f47a1e2700eb9cb8.story - time taken: 0.6658387184143066\n",
      "Processed file 83 - 64a1702a53529e9fdca82d6a5c3d5963c59c6133.story - time taken: 0.5570039749145508\n",
      "Processed file 84 - 8fab5ce2d7724f81772668c2bc058fc0920b7321.story - time taken: 0.5464358329772949\n",
      "Processed file 85 - 3dadef7ad1a1538a25e4297ee5c9d241f67a44d9.story - time taken: 0.5157308578491211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../cnn/stories\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Process stories to get generated and reference summaries\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m generated_summaries, reference_summaries \u001B[38;5;241m=\u001B[39m \u001B[43mbp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_stories\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_sentences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Calculate ROUGE scores for the summaries\u001B[39;00m\n\u001B[1;32m     29\u001B[0m rouge_scores \u001B[38;5;241m=\u001B[39m calculate_rouge_scores(generated_summaries, reference_summaries)\n",
      "File \u001B[0;32m~/Downloads/Project/cnn-text-summarisation/basic-text-summarisation/batch_processing.py:46\u001B[0m, in \u001B[0;36mprocess_stories\u001B[0;34m(directory, num_sentences, limit)\u001B[0m\n\u001B[1;32m     43\u001B[0m story_text, reference_summary, tokenizer, model, device \u001B[38;5;241m=\u001B[39m pp\u001B[38;5;241m.\u001B[39mpreprocess_story_file(story_path)\n\u001B[1;32m     44\u001B[0m sentences, tokenized_chunks \u001B[38;5;241m=\u001B[39m pp\u001B[38;5;241m.\u001B[39mpreprocess_text(story_text)\n\u001B[0;32m---> 46\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m [embedding \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m tokenized_chunks\n\u001B[1;32m     47\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m be\u001B[38;5;241m.\u001B[39mget_sentence_embeddings(chunk, tokenizer, model, device)]\n\u001B[1;32m     49\u001B[0m summary \u001B[38;5;241m=\u001B[39m sm\u001B[38;5;241m.\u001B[39msummarize_article(sentences, all_embeddings, num_sentences)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Save the summary and reference summary with proper labels\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/Project/cnn-text-summarisation/basic-text-summarisation/batch_processing.py:47\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     43\u001B[0m story_text, reference_summary, tokenizer, model, device \u001B[38;5;241m=\u001B[39m pp\u001B[38;5;241m.\u001B[39mpreprocess_story_file(story_path)\n\u001B[1;32m     44\u001B[0m sentences, tokenized_chunks \u001B[38;5;241m=\u001B[39m pp\u001B[38;5;241m.\u001B[39mpreprocess_text(story_text)\n\u001B[1;32m     46\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m [embedding \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m tokenized_chunks\n\u001B[0;32m---> 47\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[43mbe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_sentence_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m     49\u001B[0m summary \u001B[38;5;241m=\u001B[39m sm\u001B[38;5;241m.\u001B[39msummarize_article(sentences, all_embeddings, num_sentences)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Save the summary and reference summary with proper labels\u001B[39;00m\n",
      "File \u001B[0;32m~/Downloads/Project/cnn-text-summarisation/basic-text-summarisation/bert_embeddings.py:36\u001B[0m, in \u001B[0;36mget_sentence_embeddings\u001B[0;34m(chunk, tokenizer, model, device)\u001B[0m\n\u001B[1;32m     33\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m attention_mask\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Feed the chunk through the BERT model\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Extract and process the output to get sentence embeddings\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Mean pooling is used here to get a single vector for each input chunk\u001B[39;00m\n\u001B[1;32m     40\u001B[0m sentence_embedding \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1013\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1004\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1006\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1007\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1008\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1011\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1012\u001B[0m )\n\u001B[0;32m-> 1013\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1025\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1026\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:607\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    596\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    597\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    598\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m         output_attentions,\n\u001B[1;32m    605\u001B[0m     )\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 607\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    536\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    537\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[0;32m--> 539\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    542\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[1;32m    544\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/pytorch_utils.py:241\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001B[0m, in \u001B[0;36mBertLayer.feed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[0;32m--> 551\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    552\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:451\u001B[0m, in \u001B[0;36mBertIntermediate.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 451\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    452\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_act_fn(hidden_states)\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import batch_processing as bp\n",
    "from rouge import Rouge\n",
    "import os\n",
    "\n",
    "def calculate_rouge_scores(gen_summaries, ref_summaries):\n",
    "    \"\"\"\n",
    "    Calculates ROUGE scores for a set of generated summaries against reference summaries.\n",
    "\n",
    "    Args:\n",
    "    gen_summaries (list): A list of generated summaries.\n",
    "    ref_summaries (list): A list of reference summaries.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(gen_summaries, ref_summaries, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Specify the directory containing story files\n",
    "absolute_path = os.getcwd()\n",
    "relative_path = '../cnn/stories'\n",
    "directory = '../cnn/stories'\n",
    "\n",
    "# Process stories to get generated and reference summaries\n",
    "generated_summaries, reference_summaries = bp.process_stories(directory, num_sentences=5)\n",
    "\n",
    "# Calculate ROUGE scores for the summaries\n",
    "rouge_scores = calculate_rouge_scores(generated_summaries, reference_summaries)\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# Print the first 5 generated summaries for demonstration\n",
    "for summary in generated_summaries[:5]:\n",
    "    print(summary)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
